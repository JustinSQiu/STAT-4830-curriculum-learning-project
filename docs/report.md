[![\\ \documentclass{article} \\  \\ % if you need to pass options to natbib, use, e.g.: \\ %     \PassOptionsToPackage{numbers, compress}{natbib} \\ % before loading neurips_2024 \\  \\  \\ % ready for submission \\ \usepackage{neurips_2024} \\  \\  \\ % to compile a preprint version, e.g., for submission to arXiv, add add the \\ % [preprint] option: \\ %     \usepackage[preprint]{neurips_2024} \\  \\  \\ % to compile a camera-ready version, add the [final] option, e.g.: \\ %     \usepackage[final]{neurips_2024} \\  \\  \\ % to avoid loading the natbib package, add option nonatbib: \\ %    \usepackage[nonatbib]{neurips_2024} \\  \\  \\ \usepackage[utf8]{inputenc} % allow utf-8 input \\ \usepackage[T1]{fontenc}    % use 8-bit T1 fonts \\ \usepackage{hyperref}       % hyperlinks \\ \usepackage{url}            % simple URL typesetting \\ \usepackage{booktabs}       % professional-quality tables \\ \usepackage{amsfonts}       % blackboard math symbols \\ \usepackage{nicefrac}       % compact symbols for 1/2, etc. \\ \usepackage{microtype}      % microtypography \\ \usepackage{xcolor}         % colors \\ \usepackage{amsmath} \\ \usepackage{amssymb} \\ \usepackage{setspace} \\  \\  \\  \\  \\ \title{Efficient RL for Language Models} \\  \\  \\ % The \author macro works with any number of authors. There are two commands \\ % used to separate the names and addresses of multiple authors: \And and \AND. \\ % \\ % Using \And between authors leaves it to LaTeX to determine where to break the \\ % lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4 \\ % authors names on the first line, and the last on the second line, try using \\ % \AND instead of \And before the third author name. \\  \\  \\ \author{% \\ Justin Qiu \\   % David S.~Hippocampus\thanks{Use footnote for providing further information \\   %   about author (webpage, alternative address)---\emph{not} for acknowledging \\   %   funding agencies.} \\ \\   % Department of Computer Science\\ \\   % Cranberry-Lemon University\\ \\   % Pittsburgh, PA 15213 \\ \\   % \texttt{hippo@cs.cranberry-lemon.edu} \\ \\   % examples of more authors \\   % \And \\   % Coauthor \\ \\   % Affiliation \\ \\   % Address \\ \\   % \texttt{email} \\ \\   % \AND \\   % Coauthor \\ \\   % Affiliation \\ \\   % Address \\ \\   % \texttt{email} \\ \\   % \And \\   % Coauthor \\ \\   % Affiliation \\ \\   % Address \\ \\   % \texttt{email} \\ \\   % \And \\   % Coauthor \\ \\   % Affiliation \\ \\   % Address \\ \\   % \texttt{email} \\ \\ } \\  \\  \\ \begin{document} \\ \onehalfspacing \\  \\  \\ \maketitle \\  \\ % \begin{abstract} \\ %   The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on \\ %   both the left- and right-hand margins. Use 10~point type, with a vertical \\ %   spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered, \\ %   bold, and in point size 12. Two line spaces precede the abstract. The abstract \\ %   must be limited to one paragraph. \\ % \end{abstract} \\  \\  \\ \section{Problem Statement and Technical Approach} \\ Deepseek R1 \citep{deepseekai2025deepseekr1incentivizingreasoningcapability} recently introduced a novel way of training LLMs to reason better by using large-scale reinforcement learning on a pretrained model without prior supervised finetuning. They use GRPO (Group Relative Policy Optimization) to do RL on their language model. GRPO samples multiple outputs from the current policy at each step and computes the advantage of each output against the reward model compared to the other outputs. Rather than using a neural reward model, which is currently very common in current research, they use a rules-based reward function that takes into account things like mathematical accuracy, logical consistency, language consistency, and format. They optimize the language model with a loss function that takes into account both the advantage gained for each output from the updated policy and the KL divergence against a reference policy that prevents the model from deviating too far. GRPO might help improve performance with a sparse reward model over PPO. \\  \\ The loss function from the paper is below for convenience: \\  \\ \begin{equation} \\ \begin{aligned} \\ J_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)} \Bigg[\, & \frac{1}{G} \sum_{i=1}^G \bigg( \min \left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i, \right. \\ \\ & \left. \quad \text{clip} \left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)},\, 1 - \epsilon,\, 1 + \epsilon \right) A_i \right) \bigg) \\ \\ & - \beta D_{\text{KL}}(\pi_{\theta} \,\|\, \pi_{\text{ref}} f) \,\Bigg] \\ \end{aligned} \\ \label{eq:GRPO_loss} \\ \end{equation} \\  \\ where $D_{\text{KL}}$ is an approximation of the KL divergence and $A_i$ is the advantage normalized over the group of outputs. For my project, I will explore using parameter-efficient ways to train language models using reinforcement learning. Generally speaking, current methods update all parameters of the model during the backpropagation in the RL loop. I would like to see whether we can avoid this and achieve similar performance. Some ideas I have now include: \\ \begin{enumerate} \\     \item Updating only the first few or last few layers of the model. This has been explored with supervised finetuning methods, and researchers have found that performance can be maintained or even improved with far fewer weight updates than full finetuning \citep{lee2023surgicalfinetuningimprovesadaptation, lee2019elsadofreezinglayers}. \\     \item Selecting which layers to update during training. This has also been explored in the context of SFT \citep{ardakani-etal-2024-slimfit, liu2021autofreezeautomaticallyfreezingmodel}. \\     \item Optimization tricks that can approximate weight updates effectively, inspired from LoRA \citep{hu2021loralowrankadaptationlarge} and its variants. This has been applied recently to RLHF by \citet{sidahmed2024parameterefficientreinforcementlearning}, and they find that adapting LoRA to both training the reward model and updating the weights of the model being trained achieves comparable performance to regular RLHF. \\ \end{enumerate} \\  \\ As I experiment and do more research I'll definitely come across more ideas. I am currently planning to focus on math tasks, as it is relatively easy to work with, data is easy to find or synthetically generate, DeepSeek-R1 was meant for reasoning in the first place, and the reward model can be fairly straightforward, as math questions generally have a well defined answer.  \\  \\ This problem is important because the methods introduced by Deepseek-Math \citep{shao2024deepseekmathpushinglimitsmathematical} and Deepseek-R1(-Zero) represent shifts in how we think about post-training, and their empirical results show that their methods can be very successful even with less compute. I will measure success by finding out whether a method that involves less weight updates using the pure RL approach in Deepseek-R1-Zero can work. The main constraint on my project is compute, as even finetuning a small pretrained model can be computationally expensive and I don't have access to very much compute. It seems right now that the main thing that can go wrong is working with all of the libraries that have recently come out and trying to replicate or build on top of their methods. I spent five hours today trying to get TinyZero to train on a GPU cluster but somehow couldn't get it to work. I probably need to meet with the Professor to ask for advice on debugging this. \\  \\ In terms of data, one idea I had in mind is using some kind of curriculum learning method to train the model. This could entail training the model with progressively harder math data. I have found a lot of possible data online from sources like the ACL workshop on curriculum learning \citep{conll-2023-babylm}, recent reasoning datasets like OpenThoughts (https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k), etc. \\  \\ \section{Initial Results and Next Steps} \\ The first thing I wanted to do was to get TinyZero (https://github.com/Jiayi-Pan/TinyZero) working and try to freeze all but the first layer and see the results of training. For reference, TinyZero is a reproduction of DeepSeek R1 Zero for math tasks. Unfortunately, I couldn't get the training script running on my GPU cluster after spending 5+ hours trying to do it, and it was a nightmare of debugging issues with CUDA and wrestling with the lack of documentation. It would be extremely helpful to meet with the professor to talk about this.  \\  \\ I will try to produce tangible results by next milestone check-in. I tried to produce some working code for this milestone but it was really hard to get a minimal working example going quickly in just a jupyter notebook, and I spent hours reading through current open-source implementations like TinyZero, RAGEN (https://github.com/ZihanWang314/RAGEN/), and OpenR1 (https://github.com/huggingface/open-r1/) to understand what's going on. Also, since most of these implementations rely heavily on several frameworks like veRL (https://github.com/volcengine/verl) and TRL (https://github.com/huggingface/trl/), I also combed through those repositories, which took a long time. It seems that this project will involve heavy amounts of coding and modifying massive repositories. \\  \\ For next steps, I need to: \\ \begin{enumerate} \\     \item Actually get TinyZero properly running and train a small model (like Qwen0.5B) with it. \\     \item Modify the RL algorithm (still need to look into DPO/PPO/GRPO/what to use) to only change the weights of one layer and run TinyZero again \\     \item \textbf{Most important thing is I probably need some help / advice on debugging and working with these massive ML codebases} \\ \end{enumerate} \\  \\ \section{Self-Critique} \\ The obvious improvement for the next draft is that I need to actually get my code working. Because this is such a glaring weakness I don't think there is much else to discuss in this section. I will try to sync with the Professor about the overall direction of my paper during the individual meetings next week as well. I think the direction is very exciting and timely, but it is also very ambitious and not super well defined right now. My next step is to continue trying to get a minimal implementation for my idea, which I have already elaborated on above. \\  \\  \\  \\ \bibliographystyle{plainnat} \\ \bibliography{custom} \\  \\ \end{document} \\  \\ @misc{deepseekai2025deepseekr1incentivizingreasoningcapability, \\       title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},  \\       author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang}, \\       year={2025}, \\       eprint={2501.12948}, \\       archivePrefix={arXiv}, \\       primaryClass={cs.CL}, \\       url={https://arxiv.org/abs/2501.12948},  \\ } \\  \\ @misc{lee2023surgicalfinetuningimprovesadaptation, \\       title={Surgical Fine-Tuning Improves Adaptation to Distribution Shifts},  \\       author={Yoonho Lee and Annie S. Chen and Fahim Tajwar and Ananya Kumar and Huaxiu Yao and Percy Liang and Chelsea Finn}, \\       year={2023}, \\       eprint={2210.11466}, \\       archivePrefix={arXiv}, \\       primaryClass={cs.LG}, \\       url={https://arxiv.org/abs/2210.11466},  \\ } \\  \\ @misc{lee2019elsadofreezinglayers, \\       title={What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning},  \\       author={Jaejun Lee and Raphael Tang and Jimmy Lin}, \\       year={2019}, \\       eprint={1911.03090}, \\       archivePrefix={arXiv}, \\       primaryClass={cs.CL}, \\       url={https://arxiv.org/abs/1911.03090},  \\ } \\  \\ @inproceedings{ardakani-etal-2024-slimfit, \\     title = "{S}lim{F}it: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics", \\     author = "Ardakani, Arash  and \\       Haan, Altan  and \\       Tan, Shangyin  and \\       Popovici, Doru Thom  and \\       Cheung, Alvin  and \\       Iancu, Costin  and \\       Sen, Koushik", \\     editor = "Duh, Kevin  and \\       Gomez, Helena  and \\       Bethard, Steven", \\     booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)", \\     month = jun, \\     year = "2024", \\     address = "Mexico City, Mexico", \\     publisher = "Association for Computational Linguistics", \\     url = "https://aclanthology.org/2024.naacl-long.345/", \\     doi = "10.18653/v1/2024.naacl-long.345", \\     pages = "6218--6236", \\     abstract = "Transformer-based models, such as BERT and ViT, have achieved state-of-the-art results across different natural language processing (NLP) and computer vision (CV) tasks. However, these models are extremely memory intensive during their fine-tuning process, making them difficult to deploy on GPUs with limited memory resources. To address this issue, we introduce a new tool called SlimFit that reduces the memory requirements of these models by dynamically analyzing their training dynamics and freezing less-contributory layers during fine-tuning. The layers to freeze are chosen using a runtime inter-layer scheduling algorithm. This allows SlimFit to freeze up to 95{\%} of layers and reduce the overall on-device GPU memory usage of transformer-based models such as ViT and BERT by an average of 2.2x, across different NLP and CV benchmarks/datasets such as GLUE, SQuAD 2.0, CIFAR-10, CIFAR-100 and ImageNet with an average degradation of 0.2{\%} in accuracy. For such NLP and CV tasks, SlimFit can reduce up to 3.1x the total on-device memory usage with an accuracy degradation of only up to 0.4{\%}. As a result, while fine-tuning of ViT on ImageNet and BERT on SQuAD 2.0 with a batch size of 128 requires 3 and 2 32GB GPUs, respectively, SlimFit enables fine-tuning them on a single 32GB GPU without any significant accuracy degradation. The code of SlimFit is available at https://github.com/arashardakani/SlimFit." \\ } \\  \\ @misc{liu2021autofreezeautomaticallyfreezingmodel, \\       title={AutoFreeze: Automatically Freezing Model Blocks to Accelerate Fine-tuning},  \\       author={Yuhan Liu and Saurabh Agarwal and Shivaram Venkataraman}, \\       year={2021}, \\       eprint={2102.01386}, \\       archivePrefix={arXiv}, \\       primaryClass={cs.LG}, \\       url={https://arxiv.org/abs/2102.01386},  \\ } \\  \\ @misc{hu2021loralowrankadaptationlarge, \\       title={LoRA: Low-Rank Adaptation of Large Language Models},  \\       author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen}, \\       year={2021}, \\       eprint={2106.09685}, \\       archivePrefix={arXiv}, \\       primaryClass={cs.CL}, \\       url={https://arxiv.org/abs/2106.09685},  \\ } \\  \\ @misc{sidahmed2024parameterefficientreinforcementlearning, \\       title={Parameter Efficient Reinforcement Learning from Human Feedback},  \\       author={Hakim Sidahmed and Samrat Phatale and Alex Hutcheson and Zhuonan Lin and Zhang Chen and Zac Yu and Jarvis Jin and Simral Chaudhary and Roman Komarytsia and Christiane Ahlheim and Yonghao Zhu and Bowen Li and Saravanan Ganesh and Bill Byrne and Jessica Hoffmann and Hassan Mansoor and Wei Li and Abhinav Rastogi and Lucas Dixon}, \\       year={2024}, \\       eprint={2403.10704}, \\       archivePrefix={arXiv}, \\       primaryClass={cs.LG}, \\       url={https://arxiv.org/abs/2403.10704},  \\ } \\  \\ @misc{shao2024deepseekmathpushinglimitsmathematical, \\       title={DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models},  \\       author={Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo}, \\       year={2024}, \\       eprint={2402.03300}, \\       archivePrefix={arXiv}, \\       primaryClass={cs.CL}, \\       url={https://arxiv.org/abs/2402.03300},  \\ } \\  \\ @proceedings{conll-2023-babylm, \\     title = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning", \\     editor = "Warstadt, Alex  and \\       Mueller, Aaron  and \\       Choshen, Leshem  and \\       Wilcox, Ethan  and \\       Zhuang, Chengxu  and \\       Ciro, Juan  and \\       Mosquera, Rafael  and \\       Paranjabe, Bhargavi  and \\       Williams, Adina  and \\       Linzen, Tal  and \\       Cotterell, Ryan", \\     month = dec, \\     year = "2023", \\     address = "Singapore", \\     publisher = "Association for Computational Linguistics", \\     url = "https://aclanthology.org/2023.conll-babylm.0/" \\ }](https://latex.codecogs.com/svg.latex?%5C%5C%20%5Cdocumentclass%7Barticle%7D%20%5C%5C%20%20%5C%5C%20%25%20if%20you%20need%20to%20pass%20options%20to%20natbib%2C%20use%2C%20e.g.%3A%20%5C%5C%20%25%20%20%20%20%20%5CPassOptionsToPackage%7Bnumbers%2C%20compress%7D%7Bnatbib%7D%20%5C%5C%20%25%20before%20loading%20neurips_2024%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%25%20ready%20for%20submission%20%5C%5C%20%5Cusepackage%7Bneurips_2024%7D%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%25%20to%20compile%20a%20preprint%20version%2C%20e.g.%2C%20for%20submission%20to%20arXiv%2C%20add%20add%20the%20%5C%5C%20%25%20%5Bpreprint%5D%20option%3A%20%5C%5C%20%25%20%20%20%20%20%5Cusepackage%5Bpreprint%5D%7Bneurips_2024%7D%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%25%20to%20compile%20a%20camera-ready%20version%2C%20add%20the%20%5Bfinal%5D%20option%2C%20e.g.%3A%20%5C%5C%20%25%20%20%20%20%20%5Cusepackage%5Bfinal%5D%7Bneurips_2024%7D%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%25%20to%20avoid%20loading%20the%20natbib%20package%2C%20add%20option%20nonatbib%3A%20%5C%5C%20%25%20%20%20%20%5Cusepackage%5Bnonatbib%5D%7Bneurips_2024%7D%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%5Cusepackage%5Butf8%5D%7Binputenc%7D%20%25%20allow%20utf-8%20input%20%5C%5C%20%5Cusepackage%5BT1%5D%7Bfontenc%7D%20%20%20%20%25%20use%208-bit%20T1%20fonts%20%5C%5C%20%5Cusepackage%7Bhyperref%7D%20%20%20%20%20%20%20%25%20hyperlinks%20%5C%5C%20%5Cusepackage%7Burl%7D%20%20%20%20%20%20%20%20%20%20%20%20%25%20simple%20URL%20typesetting%20%5C%5C%20%5Cusepackage%7Bbooktabs%7D%20%20%20%20%20%20%20%25%20professional-quality%20tables%20%5C%5C%20%5Cusepackage%7Bamsfonts%7D%20%20%20%20%20%20%20%25%20blackboard%20math%20symbols%20%5C%5C%20%5Cusepackage%7Bnicefrac%7D%20%20%20%20%20%20%20%25%20compact%20symbols%20for%201%2F2%2C%20etc.%20%5C%5C%20%5Cusepackage%7Bmicrotype%7D%20%20%20%20%20%20%25%20microtypography%20%5C%5C%20%5Cusepackage%7Bxcolor%7D%20%20%20%20%20%20%20%20%20%25%20colors%20%5C%5C%20%5Cusepackage%7Bamsmath%7D%20%5C%5C%20%5Cusepackage%7Bamssymb%7D%20%5C%5C%20%5Cusepackage%7Bsetspace%7D%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%5Ctitle%7BEfficient%20RL%20for%20Language%20Models%7D%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%25%20The%20%5Cauthor%20macro%20works%20with%20any%20number%20of%20authors.%20There%20are%20two%20commands%20%5C%5C%20%25%20used%20to%20separate%20the%20names%20and%20addresses%20of%20multiple%20authors%3A%20%5CAnd%20and%20%5CAND.%20%5C%5C%20%25%20%5C%5C%20%25%20Using%20%5CAnd%20between%20authors%20leaves%20it%20to%20LaTeX%20to%20determine%20where%20to%20break%20the%20%5C%5C%20%25%20lines.%20Using%20%5CAND%20forces%20a%20line%20break%20at%20that%20point.%20So%2C%20if%20LaTeX%20puts%203%20of%204%20%5C%5C%20%25%20authors%20names%20on%20the%20first%20line%2C%20and%20the%20last%20on%20the%20second%20line%2C%20try%20using%20%5C%5C%20%25%20%5CAND%20instead%20of%20%5CAnd%20before%20the%20third%20author%20name.%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%5Cauthor%7B%25%20%5C%5C%20Justin%20Qiu%20%5C%5C%20%20%20%25%20David%20S.~Hippocampus%5Cthanks%7BUse%20footnote%20for%20providing%20further%20information%20%5C%5C%20%20%20%25%20%20%20about%20author%20(webpage%2C%20alternative%20address)---%5Cemph%7Bnot%7D%20for%20acknowledging%20%5C%5C%20%20%20%25%20%20%20funding%20agencies.%7D%20%5C%5C%20%5C%5C%20%20%20%25%20Department%20of%20Computer%20Science%5C%5C%20%5C%5C%20%20%20%25%20Cranberry-Lemon%20University%5C%5C%20%5C%5C%20%20%20%25%20Pittsburgh%2C%20PA%2015213%20%5C%5C%20%5C%5C%20%20%20%25%20%5Ctexttt%7Bhippo%40cs.cranberry-lemon.edu%7D%20%5C%5C%20%5C%5C%20%20%20%25%20examples%20of%20more%20authors%20%5C%5C%20%20%20%25%20%5CAnd%20%5C%5C%20%20%20%25%20Coauthor%20%5C%5C%20%5C%5C%20%20%20%25%20Affiliation%20%5C%5C%20%5C%5C%20%20%20%25%20Address%20%5C%5C%20%5C%5C%20%20%20%25%20%5Ctexttt%7Bemail%7D%20%5C%5C%20%5C%5C%20%20%20%25%20%5CAND%20%5C%5C%20%20%20%25%20Coauthor%20%5C%5C%20%5C%5C%20%20%20%25%20Affiliation%20%5C%5C%20%5C%5C%20%20%20%25%20Address%20%5C%5C%20%5C%5C%20%20%20%25%20%5Ctexttt%7Bemail%7D%20%5C%5C%20%5C%5C%20%20%20%25%20%5CAnd%20%5C%5C%20%20%20%25%20Coauthor%20%5C%5C%20%5C%5C%20%20%20%25%20Affiliation%20%5C%5C%20%5C%5C%20%20%20%25%20Address%20%5C%5C%20%5C%5C%20%20%20%25%20%5Ctexttt%7Bemail%7D%20%5C%5C%20%5C%5C%20%20%20%25%20%5CAnd%20%5C%5C%20%20%20%25%20Coauthor%20%5C%5C%20%5C%5C%20%20%20%25%20Affiliation%20%5C%5C%20%5C%5C%20%20%20%25%20Address%20%5C%5C%20%5C%5C%20%20%20%25%20%5Ctexttt%7Bemail%7D%20%5C%5C%20%5C%5C%20%7D%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%5Cbegin%7Bdocument%7D%20%5C%5C%20%5Conehalfspacing%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%5Cmaketitle%20%5C%5C%20%20%5C%5C%20%25%20%5Cbegin%7Babstract%7D%20%5C%5C%20%25%20%20%20The%20abstract%20paragraph%20should%20be%20indented%20%5Cnicefrac%7B1%7D%7B2%7D~inch%20(3~picas)%20on%20%5C%5C%20%25%20%20%20both%20the%20left-%20and%20right-hand%20margins.%20Use%2010~point%20type%2C%20with%20a%20vertical%20%5C%5C%20%25%20%20%20spacing%20(leading)%20of%2011~points.%20%20The%20word%20%5Ctextbf%7BAbstract%7D%20must%20be%20centered%2C%20%5C%5C%20%25%20%20%20bold%2C%20and%20in%20point%20size%2012.%20Two%20line%20spaces%20precede%20the%20abstract.%20The%20abstract%20%5C%5C%20%25%20%20%20must%20be%20limited%20to%20one%20paragraph.%20%5C%5C%20%25%20%5Cend%7Babstract%7D%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%5Csection%7BProblem%20Statement%20and%20Technical%20Approach%7D%20%5C%5C%20Deepseek%20R1%20%5Ccitep%7Bdeepseekai2025deepseekr1incentivizingreasoningcapability%7D%20recently%20introduced%20a%20novel%20way%20of%20training%20LLMs%20to%20reason%20better%20by%20using%20large-scale%20reinforcement%20learning%20on%20a%20pretrained%20model%20without%20prior%20supervised%20finetuning.%20They%20use%20GRPO%20(Group%20Relative%20Policy%20Optimization)%20to%20do%20RL%20on%20their%20language%20model.%20GRPO%20samples%20multiple%20outputs%20from%20the%20current%20policy%20at%20each%20step%20and%20computes%20the%20advantage%20of%20each%20output%20against%20the%20reward%20model%20compared%20to%20the%20other%20outputs.%20Rather%20than%20using%20a%20neural%20reward%20model%2C%20which%20is%20currently%20very%20common%20in%20current%20research%2C%20they%20use%20a%20rules-based%20reward%20function%20that%20takes%20into%20account%20things%20like%20mathematical%20accuracy%2C%20logical%20consistency%2C%20language%20consistency%2C%20and%20format.%20They%20optimize%20the%20language%20model%20with%20a%20loss%20function%20that%20takes%20into%20account%20both%20the%20advantage%20gained%20for%20each%20output%20from%20the%20updated%20policy%20and%20the%20KL%20divergence%20against%20a%20reference%20policy%20that%20prevents%20the%20model%20from%20deviating%20too%20far.%20GRPO%20might%20help%20improve%20performance%20with%20a%20sparse%20reward%20model%20over%20PPO.%20%5C%5C%20%20%5C%5C%20The%20loss%20function%20from%20the%20paper%20is%20below%20for%20convenience%3A%20%5C%5C%20%20%5C%5C%20%5Cbegin%7Bequation%7D%20%5C%5C%20%5Cbegin%7Baligned%7D%20%5C%5C%20J_%7B%5Ctext%7BGRPO%7D%7D(%5Ctheta)%20%3D%20%5Cmathbb%7BE%7D_%7Bq%20%5Csim%20P(Q)%2C%20%5C%7Bo_i%5C%7D_%7Bi%3D1%7D%5EG%20%5Csim%20%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(O%7Cq)%7D%20%5CBigg%5B%5C%2C%20%26%20%5Cfrac%7B1%7D%7BG%7D%20%5Csum_%7Bi%3D1%7D%5EG%20%5Cbigg(%20%5Cmin%20%5Cleft(%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(o_i%7Cq)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(o_i%7Cq)%7D%20A_i%2C%20%5Cright.%20%5C%5C%20%5C%5C%20%26%20%5Cleft.%20%5Cquad%20%5Ctext%7Bclip%7D%20%5Cleft(%20%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D(o_i%7Cq)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%7Bold%7D%7D%7D(o_i%7Cq)%7D%2C%5C%2C%201%20-%20%5Cepsilon%2C%5C%2C%201%20%2B%20%5Cepsilon%20%5Cright)%20A_i%20%5Cright)%20%5Cbigg)%20%5C%5C%20%5C%5C%20%26%20-%20%5Cbeta%20D_%7B%5Ctext%7BKL%7D%7D(%5Cpi_%7B%5Ctheta%7D%20%5C%2C%5C%7C%5C%2C%20%5Cpi_%7B%5Ctext%7Bref%7D%7D%20f)%20%5C%2C%5CBigg%5D%20%5C%5C%20%5Cend%7Baligned%7D%20%5C%5C%20%5Clabel%7Beq%3AGRPO_loss%7D%20%5C%5C%20%5Cend%7Bequation%7D%20%5C%5C%20%20%5C%5C%20where%20%24D_%7B%5Ctext%7BKL%7D%7D%24%20is%20an%20approximation%20of%20the%20KL%20divergence%20and%20%24A_i%24%20is%20the%20advantage%20normalized%20over%20the%20group%20of%20outputs.%20For%20my%20project%2C%20I%20will%20explore%20using%20parameter-efficient%20ways%20to%20train%20language%20models%20using%20reinforcement%20learning.%20Generally%20speaking%2C%20current%20methods%20update%20all%20parameters%20of%20the%20model%20during%20the%20backpropagation%20in%20the%20RL%20loop.%20I%20would%20like%20to%20see%20whether%20we%20can%20avoid%20this%20and%20achieve%20similar%20performance.%20Some%20ideas%20I%20have%20now%20include%3A%20%5C%5C%20%5Cbegin%7Benumerate%7D%20%5C%5C%20%20%20%20%20%5Citem%20Updating%20only%20the%20first%20few%20or%20last%20few%20layers%20of%20the%20model.%20This%20has%20been%20explored%20with%20supervised%20finetuning%20methods%2C%20and%20researchers%20have%20found%20that%20performance%20can%20be%20maintained%20or%20even%20improved%20with%20far%20fewer%20weight%20updates%20than%20full%20finetuning%20%5Ccitep%7Blee2023surgicalfinetuningimprovesadaptation%2C%20lee2019elsadofreezinglayers%7D.%20%5C%5C%20%20%20%20%20%5Citem%20Selecting%20which%20layers%20to%20update%20during%20training.%20This%20has%20also%20been%20explored%20in%20the%20context%20of%20SFT%20%5Ccitep%7Bardakani-etal-2024-slimfit%2C%20liu2021autofreezeautomaticallyfreezingmodel%7D.%20%5C%5C%20%20%20%20%20%5Citem%20Optimization%20tricks%20that%20can%20approximate%20weight%20updates%20effectively%2C%20inspired%20from%20LoRA%20%5Ccitep%7Bhu2021loralowrankadaptationlarge%7D%20and%20its%20variants.%20This%20has%20been%20applied%20recently%20to%20RLHF%20by%20%5Ccitet%7Bsidahmed2024parameterefficientreinforcementlearning%7D%2C%20and%20they%20find%20that%20adapting%20LoRA%20to%20both%20training%20the%20reward%20model%20and%20updating%20the%20weights%20of%20the%20model%20being%20trained%20achieves%20comparable%20performance%20to%20regular%20RLHF.%20%5C%5C%20%5Cend%7Benumerate%7D%20%5C%5C%20%20%5C%5C%20As%20I%20experiment%20and%20do%20more%20research%20I'll%20definitely%20come%20across%20more%20ideas.%20I%20am%20currently%20planning%20to%20focus%20on%20math%20tasks%2C%20as%20it%20is%20relatively%20easy%20to%20work%20with%2C%20data%20is%20easy%20to%20find%20or%20synthetically%20generate%2C%20DeepSeek-R1%20was%20meant%20for%20reasoning%20in%20the%20first%20place%2C%20and%20the%20reward%20model%20can%20be%20fairly%20straightforward%2C%20as%20math%20questions%20generally%20have%20a%20well%20defined%20answer.%20%20%5C%5C%20%20%5C%5C%20This%20problem%20is%20important%20because%20the%20methods%20introduced%20by%20Deepseek-Math%20%5Ccitep%7Bshao2024deepseekmathpushinglimitsmathematical%7D%20and%20Deepseek-R1(-Zero)%20represent%20shifts%20in%20how%20we%20think%20about%20post-training%2C%20and%20their%20empirical%20results%20show%20that%20their%20methods%20can%20be%20very%20successful%20even%20with%20less%20compute.%20I%20will%20measure%20success%20by%20finding%20out%20whether%20a%20method%20that%20involves%20less%20weight%20updates%20using%20the%20pure%20RL%20approach%20in%20Deepseek-R1-Zero%20can%20work.%20The%20main%20constraint%20on%20my%20project%20is%20compute%2C%20as%20even%20finetuning%20a%20small%20pretrained%20model%20can%20be%20computationally%20expensive%20and%20I%20don't%20have%20access%20to%20very%20much%20compute.%20It%20seems%20right%20now%20that%20the%20main%20thing%20that%20can%20go%20wrong%20is%20working%20with%20all%20of%20the%20libraries%20that%20have%20recently%20come%20out%20and%20trying%20to%20replicate%20or%20build%20on%20top%20of%20their%20methods.%20I%20spent%20five%20hours%20today%20trying%20to%20get%20TinyZero%20to%20train%20on%20a%20GPU%20cluster%20but%20somehow%20couldn't%20get%20it%20to%20work.%20I%20probably%20need%20to%20meet%20with%20the%20Professor%20to%20ask%20for%20advice%20on%20debugging%20this.%20%5C%5C%20%20%5C%5C%20In%20terms%20of%20data%2C%20one%20idea%20I%20had%20in%20mind%20is%20using%20some%20kind%20of%20curriculum%20learning%20method%20to%20train%20the%20model.%20This%20could%20entail%20training%20the%20model%20with%20progressively%20harder%20math%20data.%20I%20have%20found%20a%20lot%20of%20possible%20data%20online%20from%20sources%20like%20the%20ACL%20workshop%20on%20curriculum%20learning%20%5Ccitep%7Bconll-2023-babylm%7D%2C%20recent%20reasoning%20datasets%20like%20OpenThoughts%20(https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fopen-thoughts%2FOpenThoughts-114k)%2C%20etc.%20%5C%5C%20%20%5C%5C%20%5Csection%7BInitial%20Results%20and%20Next%20Steps%7D%20%5C%5C%20The%20first%20thing%20I%20wanted%20to%20do%20was%20to%20get%20TinyZero%20(https%3A%2F%2Fgithub.com%2FJiayi-Pan%2FTinyZero)%20working%20and%20try%20to%20freeze%20all%20but%20the%20first%20layer%20and%20see%20the%20results%20of%20training.%20For%20reference%2C%20TinyZero%20is%20a%20reproduction%20of%20DeepSeek%20R1%20Zero%20for%20math%20tasks.%20Unfortunately%2C%20I%20couldn't%20get%20the%20training%20script%20running%20on%20my%20GPU%20cluster%20after%20spending%205%2B%20hours%20trying%20to%20do%20it%2C%20and%20it%20was%20a%20nightmare%20of%20debugging%20issues%20with%20CUDA%20and%20wrestling%20with%20the%20lack%20of%20documentation.%20It%20would%20be%20extremely%20helpful%20to%20meet%20with%20the%20professor%20to%20talk%20about%20this.%20%20%5C%5C%20%20%5C%5C%20I%20will%20try%20to%20produce%20tangible%20results%20by%20next%20milestone%20check-in.%20I%20tried%20to%20produce%20some%20working%20code%20for%20this%20milestone%20but%20it%20was%20really%20hard%20to%20get%20a%20minimal%20working%20example%20going%20quickly%20in%20just%20a%20jupyter%20notebook%2C%20and%20I%20spent%20hours%20reading%20through%20current%20open-source%20implementations%20like%20TinyZero%2C%20RAGEN%20(https%3A%2F%2Fgithub.com%2FZihanWang314%2FRAGEN%2F)%2C%20and%20OpenR1%20(https%3A%2F%2Fgithub.com%2Fhuggingface%2Fopen-r1%2F)%20to%20understand%20what's%20going%20on.%20Also%2C%20since%20most%20of%20these%20implementations%20rely%20heavily%20on%20several%20frameworks%20like%20veRL%20(https%3A%2F%2Fgithub.com%2Fvolcengine%2Fverl)%20and%20TRL%20(https%3A%2F%2Fgithub.com%2Fhuggingface%2Ftrl%2F)%2C%20I%20also%20combed%20through%20those%20repositories%2C%20which%20took%20a%20long%20time.%20It%20seems%20that%20this%20project%20will%20involve%20heavy%20amounts%20of%20coding%20and%20modifying%20massive%20repositories.%20%5C%5C%20%20%5C%5C%20For%20next%20steps%2C%20I%20need%20to%3A%20%5C%5C%20%5Cbegin%7Benumerate%7D%20%5C%5C%20%20%20%20%20%5Citem%20Actually%20get%20TinyZero%20properly%20running%20and%20train%20a%20small%20model%20(like%20Qwen0.5B)%20with%20it.%20%5C%5C%20%20%20%20%20%5Citem%20Modify%20the%20RL%20algorithm%20(still%20need%20to%20look%20into%20DPO%2FPPO%2FGRPO%2Fwhat%20to%20use)%20to%20only%20change%20the%20weights%20of%20one%20layer%20and%20run%20TinyZero%20again%20%5C%5C%20%20%20%20%20%5Citem%20%5Ctextbf%7BMost%20important%20thing%20is%20I%20probably%20need%20some%20help%20%2F%20advice%20on%20debugging%20and%20working%20with%20these%20massive%20ML%20codebases%7D%20%5C%5C%20%5Cend%7Benumerate%7D%20%5C%5C%20%20%5C%5C%20%5Csection%7BSelf-Critique%7D%20%5C%5C%20The%20obvious%20improvement%20for%20the%20next%20draft%20is%20that%20I%20need%20to%20actually%20get%20my%20code%20working.%20Because%20this%20is%20such%20a%20glaring%20weakness%20I%20don't%20think%20there%20is%20much%20else%20to%20discuss%20in%20this%20section.%20I%20will%20try%20to%20sync%20with%20the%20Professor%20about%20the%20overall%20direction%20of%20my%20paper%20during%20the%20individual%20meetings%20next%20week%20as%20well.%20I%20think%20the%20direction%20is%20very%20exciting%20and%20timely%2C%20but%20it%20is%20also%20very%20ambitious%20and%20not%20super%20well%20defined%20right%20now.%20My%20next%20step%20is%20to%20continue%20trying%20to%20get%20a%20minimal%20implementation%20for%20my%20idea%2C%20which%20I%20have%20already%20elaborated%20on%20above.%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%20%5C%5C%20%5Cbibliographystyle%7Bplainnat%7D%20%5C%5C%20%5Cbibliography%7Bcustom%7D%20%5C%5C%20%20%5C%5C%20%5Cend%7Bdocument%7D%20%5C%5C%20%20%5C%5C%20%40misc%7Bdeepseekai2025deepseekr1incentivizingreasoningcapability%2C%20%5C%5C%20%20%20%20%20%20%20title%3D%7BDeepSeek-R1%3A%20Incentivizing%20Reasoning%20Capability%20in%20LLMs%20via%20Reinforcement%20Learning%7D%2C%20%20%5C%5C%20%20%20%20%20%20%20author%3D%7BDeepSeek-AI%20and%20Daya%20Guo%20and%20Dejian%20Yang%20and%20Haowei%20Zhang%20and%20Junxiao%20Song%20and%20Ruoyu%20Zhang%20and%20Runxin%20Xu%20and%20Qihao%20Zhu%20and%20Shirong%20Ma%20and%20Peiyi%20Wang%20and%20Xiao%20Bi%20and%20Xiaokang%20Zhang%20and%20Xingkai%20Yu%20and%20Yu%20Wu%20and%20Z.%20F.%20Wu%20and%20Zhibin%20Gou%20and%20Zhihong%20Shao%20and%20Zhuoshu%20Li%20and%20Ziyi%20Gao%20and%20Aixin%20Liu%20and%20Bing%20Xue%20and%20Bingxuan%20Wang%20and%20Bochao%20Wu%20and%20Bei%20Feng%20and%20Chengda%20Lu%20and%20Chenggang%20Zhao%20and%20Chengqi%20Deng%20and%20Chenyu%20Zhang%20and%20Chong%20Ruan%20and%20Damai%20Dai%20and%20Deli%20Chen%20and%20Dongjie%20Ji%20and%20Erhang%20Li%20and%20Fangyun%20Lin%20and%20Fucong%20Dai%20and%20Fuli%20Luo%20and%20Guangbo%20Hao%20and%20Guanting%20Chen%20and%20Guowei%20Li%20and%20H.%20Zhang%20and%20Han%20Bao%20and%20Hanwei%20Xu%20and%20Haocheng%20Wang%20and%20Honghui%20Ding%20and%20Huajian%20Xin%20and%20Huazuo%20Gao%20and%20Hui%20Qu%20and%20Hui%20Li%20and%20Jianzhong%20Guo%20and%20Jiashi%20Li%20and%20Jiawei%20Wang%20and%20Jingchang%20Chen%20and%20Jingyang%20Yuan%20and%20Junjie%20Qiu%20and%20Junlong%20Li%20and%20J.%20L.%20Cai%20and%20Jiaqi%20Ni%20and%20Jian%20Liang%20and%20Jin%20Chen%20and%20Kai%20Dong%20and%20Kai%20Hu%20and%20Kaige%20Gao%20and%20Kang%20Guan%20and%20Kexin%20Huang%20and%20Kuai%20Yu%20and%20Lean%20Wang%20and%20Lecong%20Zhang%20and%20Liang%20Zhao%20and%20Litong%20Wang%20and%20Liyue%20Zhang%20and%20Lei%20Xu%20and%20Leyi%20Xia%20and%20Mingchuan%20Zhang%20and%20Minghua%20Zhang%20and%20Minghui%20Tang%20and%20Meng%20Li%20and%20Miaojun%20Wang%20and%20Mingming%20Li%20and%20Ning%20Tian%20and%20Panpan%20Huang%20and%20Peng%20Zhang%20and%20Qiancheng%20Wang%20and%20Qinyu%20Chen%20and%20Qiushi%20Du%20and%20Ruiqi%20Ge%20and%20Ruisong%20Zhang%20and%20Ruizhe%20Pan%20and%20Runji%20Wang%20and%20R.%20J.%20Chen%20and%20R.%20L.%20Jin%20and%20Ruyi%20Chen%20and%20Shanghao%20Lu%20and%20Shangyan%20Zhou%20and%20Shanhuang%20Chen%20and%20Shengfeng%20Ye%20and%20Shiyu%20Wang%20and%20Shuiping%20Yu%20and%20Shunfeng%20Zhou%20and%20Shuting%20Pan%20and%20S.%20S.%20Li%20and%20Shuang%20Zhou%20and%20Shaoqing%20Wu%20and%20Shengfeng%20Ye%20and%20Tao%20Yun%20and%20Tian%20Pei%20and%20Tianyu%20Sun%20and%20T.%20Wang%20and%20Wangding%20Zeng%20and%20Wanjia%20Zhao%20and%20Wen%20Liu%20and%20Wenfeng%20Liang%20and%20Wenjun%20Gao%20and%20Wenqin%20Yu%20and%20Wentao%20Zhang%20and%20W.%20L.%20Xiao%20and%20Wei%20An%20and%20Xiaodong%20Liu%20and%20Xiaohan%20Wang%20and%20Xiaokang%20Chen%20and%20Xiaotao%20Nie%20and%20Xin%20Cheng%20and%20Xin%20Liu%20and%20Xin%20Xie%20and%20Xingchao%20Liu%20and%20Xinyu%20Yang%20and%20Xinyuan%20Li%20and%20Xuecheng%20Su%20and%20Xuheng%20Lin%20and%20X.%20Q.%20Li%20and%20Xiangyue%20Jin%20and%20Xiaojin%20Shen%20and%20Xiaosha%20Chen%20and%20Xiaowen%20Sun%20and%20Xiaoxiang%20Wang%20and%20Xinnan%20Song%20and%20Xinyi%20Zhou%20and%20Xianzu%20Wang%20and%20Xinxia%20Shan%20and%20Y.%20K.%20Li%20and%20Y.%20Q.%20Wang%20and%20Y.%20X.%20Wei%20and%20Yang%20Zhang%20and%20Yanhong%20Xu%20and%20Yao%20Li%20and%20Yao%20Zhao%20and%20Yaofeng%20Sun%20and%20Yaohui%20Wang%20and%20Yi%20Yu%20and%20Yichao%20Zhang%20and%20Yifan%20Shi%20and%20Yiliang%20Xiong%20and%20Ying%20He%20and%20Yishi%20Piao%20and%20Yisong%20Wang%20and%20Yixuan%20Tan%20and%20Yiyang%20Ma%20and%20Yiyuan%20Liu%20and%20Yongqiang%20Guo%20and%20Yuan%20Ou%20and%20Yuduan%20Wang%20and%20Yue%20Gong%20and%20Yuheng%20Zou%20and%20Yujia%20He%20and%20Yunfan%20Xiong%20and%20Yuxiang%20Luo%20and%20Yuxiang%20You%20and%20Yuxuan%20Liu%20and%20Yuyang%20Zhou%20and%20Y.%20X.%20Zhu%20and%20Yanhong%20Xu%20and%20Yanping%20Huang%20and%20Yaohui%20Li%20and%20Yi%20Zheng%20and%20Yuchen%20Zhu%20and%20Yunxian%20Ma%20and%20Ying%20Tang%20and%20Yukun%20Zha%20and%20Yuting%20Yan%20and%20Z.%20Z.%20Ren%20and%20Zehui%20Ren%20and%20Zhangli%20Sha%20and%20Zhe%20Fu%20and%20Zhean%20Xu%20and%20Zhenda%20Xie%20and%20Zhengyan%20Zhang%20and%20Zhewen%20Hao%20and%20Zhicheng%20Ma%20and%20Zhigang%20Yan%20and%20Zhiyu%20Wu%20and%20Zihui%20Gu%20and%20Zijia%20Zhu%20and%20Zijun%20Liu%20and%20Zilin%20Li%20and%20Ziwei%20Xie%20and%20Ziyang%20Song%20and%20Zizheng%20Pan%20and%20Zhen%20Huang%20and%20Zhipeng%20Xu%20and%20Zhongyu%20Zhang%20and%20Zhen%20Zhang%7D%2C%20%5C%5C%20%20%20%20%20%20%20year%3D%7B2025%7D%2C%20%5C%5C%20%20%20%20%20%20%20eprint%3D%7B2501.12948%7D%2C%20%5C%5C%20%20%20%20%20%20%20archivePrefix%3D%7BarXiv%7D%2C%20%5C%5C%20%20%20%20%20%20%20primaryClass%3D%7Bcs.CL%7D%2C%20%5C%5C%20%20%20%20%20%20%20url%3D%7Bhttps%3A%2F%2Farxiv.org%2Fabs%2F2501.12948%7D%2C%20%20%5C%5C%20%7D%20%5C%5C%20%20%5C%5C%20%40misc%7Blee2023surgicalfinetuningimprovesadaptation%2C%20%5C%5C%20%20%20%20%20%20%20title%3D%7BSurgical%20Fine-Tuning%20Improves%20Adaptation%20to%20Distribution%20Shifts%7D%2C%20%20%5C%5C%20%20%20%20%20%20%20author%3D%7BYoonho%20Lee%20and%20Annie%20S.%20Chen%20and%20Fahim%20Tajwar%20and%20Ananya%20Kumar%20and%20Huaxiu%20Yao%20and%20Percy%20Liang%20and%20Chelsea%20Finn%7D%2C%20%5C%5C%20%20%20%20%20%20%20year%3D%7B2023%7D%2C%20%5C%5C%20%20%20%20%20%20%20eprint%3D%7B2210.11466%7D%2C%20%5C%5C%20%20%20%20%20%20%20archivePrefix%3D%7BarXiv%7D%2C%20%5C%5C%20%20%20%20%20%20%20primaryClass%3D%7Bcs.LG%7D%2C%20%5C%5C%20%20%20%20%20%20%20url%3D%7Bhttps%3A%2F%2Farxiv.org%2Fabs%2F2210.11466%7D%2C%20%20%5C%5C%20%7D%20%5C%5C%20%20%5C%5C%20%40misc%7Blee2019elsadofreezinglayers%2C%20%5C%5C%20%20%20%20%20%20%20title%3D%7BWhat%20Would%20Elsa%20Do%3F%20Freezing%20Layers%20During%20Transformer%20Fine-Tuning%7D%2C%20%20%5C%5C%20%20%20%20%20%20%20author%3D%7BJaejun%20Lee%20and%20Raphael%20Tang%20and%20Jimmy%20Lin%7D%2C%20%5C%5C%20%20%20%20%20%20%20year%3D%7B2019%7D%2C%20%5C%5C%20%20%20%20%20%20%20eprint%3D%7B1911.03090%7D%2C%20%5C%5C%20%20%20%20%20%20%20archivePrefix%3D%7BarXiv%7D%2C%20%5C%5C%20%20%20%20%20%20%20primaryClass%3D%7Bcs.CL%7D%2C%20%5C%5C%20%20%20%20%20%20%20url%3D%7Bhttps%3A%2F%2Farxiv.org%2Fabs%2F1911.03090%7D%2C%20%20%5C%5C%20%7D%20%5C%5C%20%20%5C%5C%20%40inproceedings%7Bardakani-etal-2024-slimfit%2C%20%5C%5C%20%20%20%20%20title%20%3D%20%22%7BS%7Dlim%7BF%7Dit%3A%20Memory-Efficient%20Fine-Tuning%20of%20Transformer-based%20Models%20Using%20Training%20Dynamics%22%2C%20%5C%5C%20%20%20%20%20author%20%3D%20%22Ardakani%2C%20Arash%20%20and%20%5C%5C%20%20%20%20%20%20%20Haan%2C%20Altan%20%20and%20%5C%5C%20%20%20%20%20%20%20Tan%2C%20Shangyin%20%20and%20%5C%5C%20%20%20%20%20%20%20Popovici%2C%20Doru%20Thom%20%20and%20%5C%5C%20%20%20%20%20%20%20Cheung%2C%20Alvin%20%20and%20%5C%5C%20%20%20%20%20%20%20Iancu%2C%20Costin%20%20and%20%5C%5C%20%20%20%20%20%20%20Sen%2C%20Koushik%22%2C%20%5C%5C%20%20%20%20%20editor%20%3D%20%22Duh%2C%20Kevin%20%20and%20%5C%5C%20%20%20%20%20%20%20Gomez%2C%20Helena%20%20and%20%5C%5C%20%20%20%20%20%20%20Bethard%2C%20Steven%22%2C%20%5C%5C%20%20%20%20%20booktitle%20%3D%20%22Proceedings%20of%20the%202024%20Conference%20of%20the%20North%20American%20Chapter%20of%20the%20Association%20for%20Computational%20Linguistics%3A%20Human%20Language%20Technologies%20(Volume%201%3A%20Long%20Papers)%22%2C%20%5C%5C%20%20%20%20%20month%20%3D%20jun%2C%20%5C%5C%20%20%20%20%20year%20%3D%20%222024%22%2C%20%5C%5C%20%20%20%20%20address%20%3D%20%22Mexico%20City%2C%20Mexico%22%2C%20%5C%5C%20%20%20%20%20publisher%20%3D%20%22Association%20for%20Computational%20Linguistics%22%2C%20%5C%5C%20%20%20%20%20url%20%3D%20%22https%3A%2F%2Faclanthology.org%2F2024.naacl-long.345%2F%22%2C%20%5C%5C%20%20%20%20%20doi%20%3D%20%2210.18653%2Fv1%2F2024.naacl-long.345%22%2C%20%5C%5C%20%20%20%20%20pages%20%3D%20%226218--6236%22%2C%20%5C%5C%20%20%20%20%20abstract%20%3D%20%22Transformer-based%20models%2C%20such%20as%20BERT%20and%20ViT%2C%20have%20achieved%20state-of-the-art%20results%20across%20different%20natural%20language%20processing%20(NLP)%20and%20computer%20vision%20(CV)%20tasks.%20However%2C%20these%20models%20are%20extremely%20memory%20intensive%20during%20their%20fine-tuning%20process%2C%20making%20them%20difficult%20to%20deploy%20on%20GPUs%20with%20limited%20memory%20resources.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20new%20tool%20called%20SlimFit%20that%20reduces%20the%20memory%20requirements%20of%20these%20models%20by%20dynamically%20analyzing%20their%20training%20dynamics%20and%20freezing%20less-contributory%20layers%20during%20fine-tuning.%20The%20layers%20to%20freeze%20are%20chosen%20using%20a%20runtime%20inter-layer%20scheduling%20algorithm.%20This%20allows%20SlimFit%20to%20freeze%20up%20to%2095%7B%5C%25%7D%20of%20layers%20and%20reduce%20the%20overall%20on-device%20GPU%20memory%20usage%20of%20transformer-based%20models%20such%20as%20ViT%20and%20BERT%20by%20an%20average%20of%202.2x%2C%20across%20different%20NLP%20and%20CV%20benchmarks%2Fdatasets%20such%20as%20GLUE%2C%20SQuAD%202.0%2C%20CIFAR-10%2C%20CIFAR-100%20and%20ImageNet%20with%20an%20average%20degradation%20of%200.2%7B%5C%25%7D%20in%20accuracy.%20For%20such%20NLP%20and%20CV%20tasks%2C%20SlimFit%20can%20reduce%20up%20to%203.1x%20the%20total%20on-device%20memory%20usage%20with%20an%20accuracy%20degradation%20of%20only%20up%20to%200.4%7B%5C%25%7D.%20As%20a%20result%2C%20while%20fine-tuning%20of%20ViT%20on%20ImageNet%20and%20BERT%20on%20SQuAD%202.0%20with%20a%20batch%20size%20of%20128%20requires%203%20and%202%2032GB%20GPUs%2C%20respectively%2C%20SlimFit%20enables%20fine-tuning%20them%20on%20a%20single%2032GB%20GPU%20without%20any%20significant%20accuracy%20degradation.%20The%20code%20of%20SlimFit%20is%20available%20at%20https%3A%2F%2Fgithub.com%2Farashardakani%2FSlimFit.%22%20%5C%5C%20%7D%20%5C%5C%20%20%5C%5C%20%40misc%7Bliu2021autofreezeautomaticallyfreezingmodel%2C%20%5C%5C%20%20%20%20%20%20%20title%3D%7BAutoFreeze%3A%20Automatically%20Freezing%20Model%20Blocks%20to%20Accelerate%20Fine-tuning%7D%2C%20%20%5C%5C%20%20%20%20%20%20%20author%3D%7BYuhan%20Liu%20and%20Saurabh%20Agarwal%20and%20Shivaram%20Venkataraman%7D%2C%20%5C%5C%20%20%20%20%20%20%20year%3D%7B2021%7D%2C%20%5C%5C%20%20%20%20%20%20%20eprint%3D%7B2102.01386%7D%2C%20%5C%5C%20%20%20%20%20%20%20archivePrefix%3D%7BarXiv%7D%2C%20%5C%5C%20%20%20%20%20%20%20primaryClass%3D%7Bcs.LG%7D%2C%20%5C%5C%20%20%20%20%20%20%20url%3D%7Bhttps%3A%2F%2Farxiv.org%2Fabs%2F2102.01386%7D%2C%20%20%5C%5C%20%7D%20%5C%5C%20%20%5C%5C%20%40misc%7Bhu2021loralowrankadaptationlarge%2C%20%5C%5C%20%20%20%20%20%20%20title%3D%7BLoRA%3A%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models%7D%2C%20%20%5C%5C%20%20%20%20%20%20%20author%3D%7BEdward%20J.%20Hu%20and%20Yelong%20Shen%20and%20Phillip%20Wallis%20and%20Zeyuan%20Allen-Zhu%20and%20Yuanzhi%20Li%20and%20Shean%20Wang%20and%20Lu%20Wang%20and%20Weizhu%20Chen%7D%2C%20%5C%5C%20%20%20%20%20%20%20year%3D%7B2021%7D%2C%20%5C%5C%20%20%20%20%20%20%20eprint%3D%7B2106.09685%7D%2C%20%5C%5C%20%20%20%20%20%20%20archivePrefix%3D%7BarXiv%7D%2C%20%5C%5C%20%20%20%20%20%20%20primaryClass%3D%7Bcs.CL%7D%2C%20%5C%5C%20%20%20%20%20%20%20url%3D%7Bhttps%3A%2F%2Farxiv.org%2Fabs%2F2106.09685%7D%2C%20%20%5C%5C%20%7D%20%5C%5C%20%20%5C%5C%20%40misc%7Bsidahmed2024parameterefficientreinforcementlearning%2C%20%5C%5C%20%20%20%20%20%20%20title%3D%7BParameter%20Efficient%20Reinforcement%20Learning%20from%20Human%20Feedback%7D%2C%20%20%5C%5C%20%20%20%20%20%20%20author%3D%7BHakim%20Sidahmed%20and%20Samrat%20Phatale%20and%20Alex%20Hutcheson%20and%20Zhuonan%20Lin%20and%20Zhang%20Chen%20and%20Zac%20Yu%20and%20Jarvis%20Jin%20and%20Simral%20Chaudhary%20and%20Roman%20Komarytsia%20and%20Christiane%20Ahlheim%20and%20Yonghao%20Zhu%20and%20Bowen%20Li%20and%20Saravanan%20Ganesh%20and%20Bill%20Byrne%20and%20Jessica%20Hoffmann%20and%20Hassan%20Mansoor%20and%20Wei%20Li%20and%20Abhinav%20Rastogi%20and%20Lucas%20Dixon%7D%2C%20%5C%5C%20%20%20%20%20%20%20year%3D%7B2024%7D%2C%20%5C%5C%20%20%20%20%20%20%20eprint%3D%7B2403.10704%7D%2C%20%5C%5C%20%20%20%20%20%20%20archivePrefix%3D%7BarXiv%7D%2C%20%5C%5C%20%20%20%20%20%20%20primaryClass%3D%7Bcs.LG%7D%2C%20%5C%5C%20%20%20%20%20%20%20url%3D%7Bhttps%3A%2F%2Farxiv.org%2Fabs%2F2403.10704%7D%2C%20%20%5C%5C%20%7D%20%5C%5C%20%20%5C%5C%20%40misc%7Bshao2024deepseekmathpushinglimitsmathematical%2C%20%5C%5C%20%20%20%20%20%20%20title%3D%7BDeepSeekMath%3A%20Pushing%20the%20Limits%20of%20Mathematical%20Reasoning%20in%20Open%20Language%20Models%7D%2C%20%20%5C%5C%20%20%20%20%20%20%20author%3D%7BZhihong%20Shao%20and%20Peiyi%20Wang%20and%20Qihao%20Zhu%20and%20Runxin%20Xu%20and%20Junxiao%20Song%20and%20Xiao%20Bi%20and%20Haowei%20Zhang%20and%20Mingchuan%20Zhang%20and%20Y.%20K.%20Li%20and%20Y.%20Wu%20and%20Daya%20Guo%7D%2C%20%5C%5C%20%20%20%20%20%20%20year%3D%7B2024%7D%2C%20%5C%5C%20%20%20%20%20%20%20eprint%3D%7B2402.03300%7D%2C%20%5C%5C%20%20%20%20%20%20%20archivePrefix%3D%7BarXiv%7D%2C%20%5C%5C%20%20%20%20%20%20%20primaryClass%3D%7Bcs.CL%7D%2C%20%5C%5C%20%20%20%20%20%20%20url%3D%7Bhttps%3A%2F%2Farxiv.org%2Fabs%2F2402.03300%7D%2C%20%20%5C%5C%20%7D%20%5C%5C%20%20%5C%5C%20%40proceedings%7Bconll-2023-babylm%2C%20%5C%5C%20%20%20%20%20title%20%3D%20%22Proceedings%20of%20the%20BabyLM%20Challenge%20at%20the%2027th%20Conference%20on%20Computational%20Natural%20Language%20Learning%22%2C%20%5C%5C%20%20%20%20%20editor%20%3D%20%22Warstadt%2C%20Alex%20%20and%20%5C%5C%20%20%20%20%20%20%20Mueller%2C%20Aaron%20%20and%20%5C%5C%20%20%20%20%20%20%20Choshen%2C%20Leshem%20%20and%20%5C%5C%20%20%20%20%20%20%20Wilcox%2C%20Ethan%20%20and%20%5C%5C%20%20%20%20%20%20%20Zhuang%2C%20Chengxu%20%20and%20%5C%5C%20%20%20%20%20%20%20Ciro%2C%20Juan%20%20and%20%5C%5C%20%20%20%20%20%20%20Mosquera%2C%20Rafael%20%20and%20%5C%5C%20%20%20%20%20%20%20Paranjabe%2C%20Bhargavi%20%20and%20%5C%5C%20%20%20%20%20%20%20Williams%2C%20Adina%20%20and%20%5C%5C%20%20%20%20%20%20%20Linzen%2C%20Tal%20%20and%20%5C%5C%20%20%20%20%20%20%20Cotterell%2C%20Ryan%22%2C%20%5C%5C%20%20%20%20%20month%20%3D%20dec%2C%20%5C%5C%20%20%20%20%20year%20%3D%20%222023%22%2C%20%5C%5C%20%20%20%20%20address%20%3D%20%22Singapore%22%2C%20%5C%5C%20%20%20%20%20publisher%20%3D%20%22Association%20for%20Computational%20Linguistics%22%2C%20%5C%5C%20%20%20%20%20url%20%3D%20%22https%3A%2F%2Faclanthology.org%2F2023.conll-babylm.0%2F%22%20%5C%5C%20%7D)](#_)